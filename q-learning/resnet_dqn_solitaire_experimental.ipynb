{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Solitaire DQN, Pytorch Implementation using FC DNN"
      ],
      "metadata": {
        "id": "x3hCpKGeV-ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hardware Check"
      ],
      "metadata": {
        "id": "N6cXdnp9WLhU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRPAqotPDbvy",
        "outputId": "97e15f4e-f04d-44ae-8d20-559925c4943f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 22 23:26:18 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    26W /  70W |   1344MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOu0uJuiDg-n",
        "outputId": "7c598d8f-8fd1-470d-dcdd-f0a7c3f49e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "4D2Y9LGLWNz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "di3Paea2Dixs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solitaire Gym Class and Functions"
      ],
      "metadata": {
        "id": "-0p6DhRUD_lI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.spaces import Tuple, Discrete, MultiDiscrete, Sequence\n",
        "from gym.error import DependencyNotInstalled\n",
        "\n",
        "#credit: https://stackoverflow.com/questions/47269390/how-to-find-first-non-zero-value-in-every-column-of-a-numpy-array\n",
        "def first_nonzero_index(array):\n",
        "    \"\"\"Return the index of the first non-zero element of array. If all elements are zero, return -1.\"\"\"\n",
        "    \n",
        "    fnzi = -1 # first non-zero index\n",
        "    indices = np.flatnonzero(array)\n",
        "       \n",
        "    if (len(indices) > 0):\n",
        "        fnzi = indices[0]\n",
        "        \n",
        "    return fnzi\n",
        "\n",
        "#gets suit number given card num (worried about it always returning 0, shouldnt it be -1?)\n",
        "def get_suit(card_num):\n",
        "    if card_num / 13 <= 1:\n",
        "        return 0\n",
        "    elif card_num / 26 <= 1:\n",
        "        return 1\n",
        "    elif card_num / 39 <= 1:\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n",
        "    return 0\n",
        "\n",
        "#gets suit and card num, given card num\n",
        "def get_suit_and_num(card_num):\n",
        "    if card_num == 0 or card_num == 53:\n",
        "        return [0, card_num]\n",
        "\n",
        "    suit = 0\n",
        "    num = card_num % 13\n",
        "    if num == 0:\n",
        "        num = 13\n",
        "\n",
        "    if card_num / 13 <= 1:\n",
        "        suit = 0\n",
        "    elif card_num / 26 <= 1:\n",
        "        suit = 1\n",
        "    elif card_num / 39 <= 1:\n",
        "        suit = 2\n",
        "    else:\n",
        "        suit = 3\n",
        "\n",
        "    return [suit, num]\n",
        "\n",
        "#gets character from the suit (for displaying images)\n",
        "def get_suit_char_from_val(suit_val):\n",
        "    if suit_val == 0:\n",
        "        return \"H\"\n",
        "    elif suit_val == 1:\n",
        "        return \"D\"\n",
        "    elif suit_val == 2:\n",
        "        return \"S\"\n",
        "    elif suit_val == 3:\n",
        "        return \"C\"\n",
        "\n",
        "#gets character from the card num (for displaying images)\n",
        "def get_card_char(card_num):\n",
        "    if card_num == 1:\n",
        "        return \"A\"\n",
        "    elif card_num == 10:\n",
        "        return \"T\"\n",
        "    elif card_num == 11:\n",
        "        return \"J\"\n",
        "    elif card_num == 12:\n",
        "        return \"Q\"\n",
        "    elif card_num == 13:\n",
        "        return \"K\"\n",
        "    else:\n",
        "        return str(int(card_num))\n",
        "\n",
        "#returns an np array size 52 with the numbers 1-53(non-inc), False-resampled\n",
        "def get_shuffled_deck(np_random):\n",
        "    return np_random.choice(range(1,53), 52, False)\n",
        "\n",
        "#checks if can move card from deck to suit\n",
        "def deck_to_suit_check(deck_cards_param, suit_cards_param, highest_nonzero_deck):\n",
        "    #-1 is encoded in this variable as the row is entirely empty, so cant move anything from empty deck top row\n",
        "    if highest_nonzero_deck > -1:\n",
        "        #make sure it can slot into the pile properly\n",
        "        active_deck_card = deck_cards_param[0, highest_nonzero_deck]\n",
        "        active_deck_card_suit_and_num = get_suit_and_num(active_deck_card)\n",
        "        a_suit = active_deck_card_suit_and_num[0]\n",
        "        a_num = active_deck_card_suit_and_num[1]\n",
        "        #if suit cards is one below active deck card number, then can add to suits\n",
        "        if suit_cards_param[a_suit] + 1 == a_num:\n",
        "            return True, a_suit\n",
        "    return False, False\n",
        "\n",
        "#checks if can move from deck to pile\n",
        "def deck_to_pile_check(deck_cards_param, pile_cards_param, pile_i, highest_nonzero_deck):\n",
        "    #-1 is encoded in this variable as the row is entirely empty, so cant move anything from empty deck top row\n",
        "    if highest_nonzero_deck > -1:\n",
        "        #get suit and num of the deck card\n",
        "        c_suit_and_num = get_suit_and_num(deck_cards_param[0, highest_nonzero_deck])\n",
        "        c_suit = c_suit_and_num[0]\n",
        "        c_num = c_suit_and_num[1]\n",
        "\n",
        "        #check if deck card is king, if so just check if pile bottom most is empty\n",
        "        if c_num == 13:\n",
        "            if pile_cards_param[0,pile_i] == 0:\n",
        "                return True\n",
        "        else:\n",
        "\n",
        "            #check pile bottom most is not empty\n",
        "            if pile_cards_param[0, pile_i] > 0:\n",
        "                \n",
        "                #check the pile you are trying to move to's bottom most card is one lower and different suit\n",
        "                t_suit_and_num = get_suit_and_num(pile_cards_param[0,pile_i])\n",
        "                \n",
        "                #check opposite suits and current card is one lower than target\n",
        "                if ((c_suit <= 1 and t_suit_and_num[0] >= 2) or (c_suit >= 2 and t_suit_and_num[0] <= 1)) and (c_num + 1 == t_suit_and_num[1]):\n",
        "                    return True  \n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "#checks if can move from suit down to pile\n",
        "def suit_to_pile_check(suit_cards_param, pile_cards_param, pile_i, suit_j):\n",
        "    #check suit card you are trying to move exists, i.e. greater than zero\n",
        "    #and also greater than one because why consider moving ace down?\n",
        "    c_num = suit_cards_param[suit_j]\n",
        "    if c_num > 1:\n",
        "        \n",
        "        #first, check it isnt a king edge case\n",
        "        if c_num == 13:\n",
        "            \n",
        "            #check if pile trying to move to is empty\n",
        "            if pile_cards_param[0, pile_i] == 0:\n",
        "                return True\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            #check pile bottom most is not empty\n",
        "            if pile_cards_param[0, pile_i] > 0:\n",
        "                \n",
        "                #check the pile you are trying to move to's bottom most card is one lower and different suit\n",
        "                t_suit_and_num = get_suit_and_num(pile_cards_param[0,pile_i])\n",
        "                \n",
        "                #check opposite suits and current card is one lower than target\n",
        "                if ((suit_j <= 1 and t_suit_and_num[0] >= 2) or (suit_j >= 2 and t_suit_and_num[0] <= 1)) and (c_num + 1 == t_suit_and_num[1]):\n",
        "                    return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "#check if can move from pile to suit\n",
        "def pile_to_suit_check(pile_cards_param, suit_cards_param, pile_i):\n",
        "    #check there is a card in the pile\n",
        "    if pile_cards_param[0,pile_i] > 0:\n",
        "        #get suit and number of bottom most pile card\n",
        "        a_suit_and_num = get_suit_and_num(pile_cards_param[0,pile_i])\n",
        "        #check that the card is indeed one higher than what is current in its suit\n",
        "        if a_suit_and_num[1] - 1 == suit_cards_param[a_suit_and_num[0]]:\n",
        "            return True, a_suit_and_num[0]\n",
        "    return False, False\n",
        "\n",
        "\n",
        "#check that agent can move some card index (and all the cards below it) to\n",
        "#another pile\n",
        "#index 0 is lowest card\n",
        "def pile_to_pile_check(pile_cards_param, pile_i, pile_to_move_to_j, card_k, highest_nonzero_pile_i):\n",
        "    #check current pile is not empty\n",
        "    if highest_nonzero_pile_i > -1:\n",
        "        #check that there is a card which is a number \n",
        "        #of cards below the top one you want to move\n",
        "        #i.e. if the pile is ace,2,3,4, and card_k=7, that is out of bounds\n",
        "        #max would be card_k = 3, which would move ace. card_k = 0 moves card 4\n",
        "        #only 12 actions because in k,q,j,10,9,8,7,6,5,4,3,2,ace, it would only\n",
        "        #ever make sense to go down to the 2 and move that, otherwise\n",
        "        #send the ace to the suits\n",
        "        #highest nonzero pile i here should be 3\n",
        "        if card_k <= highest_nonzero_pile_i:\n",
        "            #get the index of the current card you want to move. In the above example\n",
        "            #c_k = 3 would mean we move index 0, the ace, exactly. c_k=0 means we move\n",
        "            #index 3, the 4, exactly\n",
        "            index_to_move = highest_nonzero_pile_i - card_k\n",
        "            #get the current top card trying to move's suit and num\n",
        "            c_suit_and_num = get_suit_and_num(pile_cards_param[index_to_move,pile_i])\n",
        "\n",
        "            #if the current card we want to move is a king, check the target pile is empty\n",
        "            if c_suit_and_num[1] == 13:\n",
        "                if not pile_cards_param[:,pile_to_move_to_j].any():\n",
        "                    return True\n",
        "            else:\n",
        "                #else its some other card, so check the pile we want to move to has\n",
        "                #card at index 0, and that it is one lower and different suit than current\n",
        "                if pile_cards_param[0,pile_to_move_to_j] > 0:\n",
        "                    t_suit_and_num = get_suit_and_num(pile_cards_param[0,pile_to_move_to_j])\n",
        "                    #check diff suit and one higher\n",
        "                    if ((c_suit_and_num[0] <= 1 and t_suit_and_num[0] >= 2) or (c_suit_and_num[0] >= 2 and t_suit_and_num[0] <= 1)) and (c_suit_and_num[1] + 1 == t_suit_and_num[1]):\n",
        "                        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "#solitaire world environment class\n",
        "class SolitaireWorldEnv(gym.Env):\n",
        "\n",
        "    metadata = {\n",
        "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
        "        \"render_fps\": 4,\n",
        "    }\n",
        "\n",
        "    def __init__(self, render_mode: Optional[str] = None, natural=False, sab=False):\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        #Remember, obs are only thing agent can see. Other self... things can\n",
        "        #exist but not be in here, to manage underlying but unknown-to-agent state\n",
        "\n",
        "        #8 decks to flip thru, 3 cards in a deck, can be 53 (when it initializes and\n",
        "        #agent hasn't flipped through, but once it has just becomes 0-52 I guess\n",
        "        decks_obs_shape = np.zeros((8,3), dtype=np.int8)\n",
        "        decks_obs_shape.fill(54)\n",
        "        #13 possible cards in a pile, 7 piles, 0 unknown, 1-52 card\n",
        "        piles_obs_shape = np.zeros((13,7), dtype=np.int8)\n",
        "        piles_obs_shape.fill(53)\n",
        "        #piles behind has 7 piles, with as many as 6 cards behind\n",
        "        piles_behind_obs_shape = np.zeros((6,7), dtype=np.int8)\n",
        "        piles_behind_obs_shape.fill(54)\n",
        "\n",
        "        piles_behind_actual_obs_shape = np.zeros((6,7), dtype=np.int8)\n",
        "        piles_behind_actual_obs_shape.fill(53)\n",
        "\n",
        "        self.action_space = Discrete(548)\n",
        "        self.observation_space = spaces.Dict({\n",
        "            \"deck_position\": Discrete(8),\n",
        "            \"decks_actual\": MultiDiscrete(decks_obs_shape), #for testing\n",
        "            \"decks\": MultiDiscrete(decks_obs_shape),\n",
        "            \"suits\": MultiDiscrete(np.array([14,14,14,14])),\n",
        "            \"piles\": MultiDiscrete(piles_obs_shape),\n",
        "            \"piles_behind\": MultiDiscrete(piles_behind_obs_shape), #6 possible cards behind\n",
        "            \"piles_behind_actual\": MultiDiscrete(piles_behind_actual_obs_shape) #for testing\n",
        "        })\n",
        "\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    #returns array of 0s and 1s, of length num actions,\n",
        "    #where if the value at the index in the array for the action is 1\n",
        "    #the action is valid\n",
        "    def action_mask(self, deck_cards_p, suit_cards_p, pile_cards_p):\n",
        "        mask = np.zeros(548, dtype=np.int8)\n",
        "        #can always tap the deck\n",
        "        mask[0] = 1\n",
        "        #gets the highest nonzero index by rows, for the first row of the deck array\n",
        "        #credit: https://stackoverflow.com/questions/67921398/find-the-index-of-last-non-zero-value-per-column-of-a-2-d-array\n",
        "        #deck first nonzero, obviously we only care about row 0, hence [0] at end\n",
        "        highest_nonzero_deck_am = np.where(np.count_nonzero(deck_cards_p, axis=1)==0, -1, (deck_cards_p.shape[1]-1) - np.argmin(deck_cards_p[:,::-1]==0, axis=1))[0]\n",
        "        #need highest nonzero, since needed for checking pile to pile actions\n",
        "        highest_nonzeros_piles_am = np.where(np.count_nonzero(pile_cards_p, axis=0)==0, -1, (pile_cards_p.shape[0]-1) - np.argmin(pile_cards_p[::-1,:]==0, axis=0))\n",
        "\n",
        "        #for each action, besides 0 because always possible to tap deck\n",
        "        for i in range(1,548):\n",
        "            if i == 1:\n",
        "                can_deck_to_suit, _ = deck_to_suit_check(deck_cards_p, suit_cards_p, highest_nonzero_deck_am)\n",
        "                if can_deck_to_suit:\n",
        "                    mask[i] = 1\n",
        "\n",
        "            elif i >= 2 and i <= 8:\n",
        "                p_i = i - 2\n",
        "                can_deck_to_pile = deck_to_pile_check(deck_cards_p, pile_cards_p, p_i, highest_nonzero_deck_am)\n",
        "                if can_deck_to_pile:\n",
        "                    mask[i] = 1\n",
        "\n",
        "            elif i >= 9 and i <= 36:\n",
        "                p_i = (i - 9) % 7\n",
        "                s_j = (i - 9) // 7\n",
        "                can_suit_to_pile = suit_to_pile_check(suit_cards_p, pile_cards_p, p_i, s_j)\n",
        "                if can_suit_to_pile:\n",
        "                    mask[i] = 1\n",
        "\n",
        "            elif i >= 37 and i <= 43:\n",
        "                p_i = (i - 37)\n",
        "                can_pile_to_suit, _ = pile_to_suit_check(pile_cards_p, suit_cards_p, p_i)\n",
        "                if can_pile_to_suit:\n",
        "                    mask[i] = 1\n",
        "\n",
        "            elif i >= 44 and i <= 547:\n",
        "                p_i = (i - 44) // 72\n",
        "                p_to_m = int(((i - 44 - (72*p_i)) // 12) + 1)\n",
        "                p_to_m_j = (p_i + p_to_m) % 7\n",
        "                c_k = (i - 44) % 12\n",
        "                can_pile_to_pile = pile_to_pile_check(pile_cards_p, p_i, p_to_m_j, c_k, highest_nonzeros_piles_am[p_i])\n",
        "                if can_pile_to_pile:\n",
        "                    mask[i] = 1\n",
        "\n",
        "        return mask\n",
        "\n",
        "    #gym step func\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action)\n",
        "\n",
        "        terminated = False\n",
        "        #game reward for initial part of step starts at zero, changes depending on action outcome\n",
        "        game_reward = 0\n",
        "        #agent reward however is -1 because we want to penalize it for steping, since time is a factor\n",
        "        reward = 0\n",
        "\n",
        "        #source: https://en.wikipedia.org/wiki/Klondike_(solitaire) Microsoft Windows Scoring section\n",
        "        game_score_deck_to_pile = 5\n",
        "        game_score_deck_to_suit = 10\n",
        "        game_score_pile_to_suit = 10\n",
        "        game_score_pile_card_reveal = 5\n",
        "        game_score_suit_to_pile = -15\n",
        "        game_score_deck_cycle = -20\n",
        "        game_score_victory = 10000\n",
        "\n",
        "        agent_reward_deck_to_pile = 1\n",
        "        agent_reward_deck_to_suit = 2\n",
        "        agent_reward_pile_to_suit = 1\n",
        "        agent_reward_pile_to_pile = 0\n",
        "        agent_reward_pile_card_reveal = 0.5\n",
        "        agent_reward_suit_to_pile = -1.25\n",
        "        agent_reward_deck_cycle = 0\n",
        "        agent_reward_deck_flip = 0\n",
        "        agent_reward_victory = 10\n",
        "\n",
        "\n",
        "        #action 0 is tapping deck\n",
        "        if action == 0:\n",
        "            #append first row to end, for underlying and known sets\n",
        "            self.deck_cards = np.append(self.deck_cards, [self.deck_cards[0,:]], axis=0)\n",
        "            self.deck_cards_known = np.append(self.deck_cards_known, [self.deck_cards_known[0,:]], axis=0)\n",
        "            #then delete first row for both\n",
        "            self.deck_cards = np.delete(self.deck_cards, (0), axis=0)\n",
        "            self.deck_cards_known = np.delete(self.deck_cards_known, (0), axis=0)\n",
        "            #and copy the top row of deck cards to deck cards known, since we know it now\n",
        "            self.deck_cards_known[0,:] = self.deck_cards[0,:]\n",
        "\n",
        "            if self.deck_position == 7:\n",
        "                #flatten, put zeros at end, reshape\n",
        "                #part of rules to how deck works, weird\n",
        "                flat_deck = self.deck_cards.flatten()\n",
        "                #count occurances of 0's\n",
        "                num_zeros = np.count_nonzero(flat_deck == 0)\n",
        "                #remove zeros\n",
        "                flat_deck_no_zeros = np.delete(flat_deck, np.where(flat_deck == 0), axis = -1)\n",
        "                #add them to end\n",
        "                flat_deck_zeros = np.append(flat_deck_no_zeros, np.zeros(num_zeros, dtype=np.int8), axis = -1)\n",
        "                #update state\n",
        "                self.deck_cards = flat_deck_zeros.reshape((8,3))\n",
        "                #at this point, since we have cycled through, we now have complete information and can just copy\n",
        "                self.deck_cards_known = self.deck_cards\n",
        "                self.deck_position = 0\n",
        "                #no reward, unless its the 7th\n",
        "                game_reward += game_score_deck_cycle\n",
        "                reward += agent_reward_deck_cycle\n",
        "            else:\n",
        "                reward += agent_reward_deck_flip\n",
        "                #increase deck position (so to know when to reset deck and penalize)\n",
        "                self.deck_position = self.deck_position + 1\n",
        "        \n",
        "        #action 1 is tapping active deck card, attempting to sent to suit\n",
        "        elif action == 1:\n",
        "            highest_nonzero_deck = np.where(np.count_nonzero(self.deck_cards, axis=1)==0, -1, (self.deck_cards.shape[1]-1) - np.argmin(self.deck_cards[:,::-1]==0, axis=1))[0]\n",
        "            can_deck_to_suit, deck_card_suit = deck_to_suit_check(self.deck_cards, self.suit_cards, highest_nonzero_deck)\n",
        "            if can_deck_to_suit:\n",
        "                #update suit cards\n",
        "                self.suit_cards[deck_card_suit] = self.suit_cards[deck_card_suit] + 1\n",
        "                #set card val to 0 for empty at the given index\n",
        "                self.deck_cards[0,highest_nonzero_deck] = 0\n",
        "                self.deck_cards_known[0,highest_nonzero_deck] = 0\n",
        "                game_reward += agent_reward_deck_to_suit\n",
        "                reward += agent_reward_deck_to_suit\n",
        "\n",
        "                #check if terminal/goal state, all suits at king\n",
        "                if self.suit_cards[0] == 13 and self.suit_cards[1] == 13 and self.suit_cards[2] == 13 and self.suit_cards[3] == 13:\n",
        "                    print(\"VICTORY!\")\n",
        "                    print(self.suit_cards)\n",
        "                    terminated = True\n",
        "                    game_reward += game_score_victory\n",
        "                    reward += agent_reward_victory\n",
        "    \n",
        "        #actions 2-8 are trying to move top deck card to one of 7 other piles\n",
        "        elif action >= 2 and action <= 8:\n",
        "            pile_i = action - 2\n",
        "            highest_nonzero_deck = np.where(np.count_nonzero(self.deck_cards, axis=1)==0, -1, (self.deck_cards.shape[1]-1) - np.argmin(self.deck_cards[:,::-1]==0, axis=1))[0]\n",
        "\n",
        "            can_deck_to_pile = deck_to_pile_check(self.deck_cards, self.pile_cards, pile_i, highest_nonzero_deck)      \n",
        "            if can_deck_to_pile:\n",
        "                #shift all cards one index up to make room for this at bottom (index 0)\n",
        "                self.pile_cards[1:,pile_i] = self.pile_cards[:-1,pile_i]\n",
        "                #set bottom most index card to be the one we added, since we can now\n",
        "                self.pile_cards[0,pile_i] = self.deck_cards[0,highest_nonzero_deck]\n",
        "\n",
        "                #set deck card val to 0 for empty at the given index\n",
        "                self.deck_cards[0,highest_nonzero_deck] = 0\n",
        "                self.deck_cards_known[0,highest_nonzero_deck] = 0\n",
        "\n",
        "                #deck to pile rewards\n",
        "                game_reward += game_score_deck_to_pile\n",
        "                reward += agent_reward_deck_to_pile\n",
        "        \n",
        "        #actions 9-15 are trying to move suit's 0 (hearts) to one of 7 other piles\n",
        "        #actions 16-22 are trying to move suit's 1 (diamonds) to one of 7 other piles\n",
        "        #...etc\n",
        "        elif action >= 9 and action <= 36:\n",
        "            pile_i = (action - 9) % 7\n",
        "            suit_j = (action - 9) // 7\n",
        "\n",
        "            can_suit_to_pile = suit_to_pile_check(self.suit_cards, self.pile_cards, pile_i, suit_j)\n",
        "\n",
        "            if can_suit_to_pile:\n",
        "                #shift pile cards up, to make room for suit card to add\n",
        "                self.pile_cards[1:,pile_i] = self.pile_cards[:-1,pile_i]\n",
        "                #set bottom most index card to be the one we added, since we can now\n",
        "                #have to multiply by suit_j*13 to get original card val back from suit and num\n",
        "                self.pile_cards[0,pile_i] = int((suit_j * 13) + self.suit_cards[suit_j])\n",
        "\n",
        "                #reduce suit card val, since we took a card off it\n",
        "                self.suit_cards[suit_j] -= 1\n",
        "\n",
        "                #suit to pile rewards (negative)\n",
        "                game_reward += game_score_suit_to_pile\n",
        "                reward += agent_reward_suit_to_pile\n",
        "       \n",
        "        #actions 37-43 are trying to move bottom-most card in one of 7 piles\n",
        "        #to their given suits\n",
        "        elif action >= 37 and action <= 43:\n",
        "            pile_i = (action - 37)\n",
        "\n",
        "            can_pile_to_suit, pile_card_suit = pile_to_suit_check(self.pile_cards, self.suit_cards, pile_i)\n",
        "\n",
        "            if can_pile_to_suit:\n",
        "                #update suit cards\n",
        "                self.suit_cards[pile_card_suit] += 1\n",
        "\n",
        "                #update pile cards\n",
        "                #(set cards from beginning to one from end to the cards from 1 to end)\n",
        "                self.pile_cards[:-1,pile_i] = self.pile_cards[1:,pile_i]\n",
        "                #and set end card to 0, since we removed a card from the pile\n",
        "                self.pile_cards[-1,pile_i] = 0\n",
        "\n",
        "                #pile to suit reward\n",
        "                game_reward += game_score_pile_to_suit\n",
        "                reward += agent_reward_pile_to_suit\n",
        "\n",
        "                #check if the pile is now empty, and there are upside-down cards behind it\n",
        "                #if so, turn it up and add it to correct spot in pile_cards\n",
        "                if not self.pile_cards[:,pile_i].any():\n",
        "                    #get highest nonzero for pile behind card\n",
        "                    highest_nonzeros_piles_behind = np.where(np.count_nonzero(self.pile_behind_cards, axis=0)==0, -1, (self.pile_behind_cards.shape[0]-1) - np.argmin(self.pile_behind_cards[::-1,:]==0, axis=0))[pile_i]\n",
        "                    if highest_nonzeros_piles_behind > -1:\n",
        "                        self.pile_cards[0, pile_i] = self.pile_behind_cards[highest_nonzeros_piles_behind,pile_i]\n",
        "                        self.pile_behind_cards[highest_nonzeros_piles_behind, pile_i] = 0\n",
        "                        self.pile_behind_cards_known[highest_nonzeros_piles_behind, pile_i] = 0\n",
        "                        game_reward += game_score_pile_card_reveal\n",
        "                        reward += agent_reward_pile_card_reveal      \n",
        "\n",
        "                #check if terminal/goal state, all suits at king\n",
        "                if self.suit_cards[0] == 13 and self.suit_cards[1] == 13 and self.suit_cards[2] == 13 and self.suit_cards[3] == 13:\n",
        "                    print(\"VICTORY!\")\n",
        "                    print(self.suit_cards)\n",
        "                    terminated = True\n",
        "                    game_reward += game_score_victory\n",
        "                    reward += agent_reward_victory\n",
        "\n",
        "        #actions 44 to 547 are pile to pile moves, defined by the vars below\n",
        "        elif action >= 44 and action <= 547:\n",
        "            pile_i = (action - 44) // 72\n",
        "            piles_to_move = int(((action - 44 - (72*pile_i)) // 12) + 1)\n",
        "            #print(\"piles to move: \", piles_to_move)\n",
        "            pile_to_move_to_j = (pile_i + piles_to_move) % 7\n",
        "            card_k = (action - 44) % 12\n",
        "\n",
        "            highest_nonzeros_piles = np.where(np.count_nonzero(self.pile_cards, axis=0)==0, -1, (self.pile_cards.shape[0]-1) - np.argmin(self.pile_cards[::-1,:]==0, axis=0))\n",
        "            highest_nonzero_pile_i = highest_nonzeros_piles[pile_i]\n",
        "            can_pile_to_pile = pile_to_pile_check(self.pile_cards, pile_i, pile_to_move_to_j, card_k, highest_nonzero_pile_i)\n",
        "\n",
        "            if can_pile_to_pile:\n",
        "                #if can pile to pile,\n",
        "                #card_k = 0, highest_nonzero_pile_i = 4, we are moving 4 cards to pile j, and these cards go under the others at pile j\n",
        "                #so pile j needs to shift cards up by highest_nzpi - c_k, i.e. 4\n",
        "                #+1 because ...?\n",
        "                reward += agent_reward_pile_to_pile\n",
        "\n",
        "                index_to_move = highest_nonzero_pile_i - card_k + 1\n",
        "\n",
        "                #first, shift cards up\n",
        "                self.pile_cards[index_to_move:,pile_to_move_to_j] = self.pile_cards[:(13 - index_to_move),pile_to_move_to_j]\n",
        "                #then add in cards from pile i\n",
        "                self.pile_cards[:index_to_move,pile_to_move_to_j] = self.pile_cards[:index_to_move,pile_i]\n",
        "                #finally shift pile i cards down\n",
        "                self.pile_cards[:(13 - index_to_move),pile_i] = self.pile_cards[index_to_move:,pile_i]\n",
        "                #and zero out rest of pile\n",
        "                self.pile_cards[(13 - index_to_move):,pile_i] = 0 \n",
        "\n",
        "                #if pile is now empty that we removed some cards, i.e. we removed top card because card k is 0, meaning no cards below it selected as top\n",
        "                if card_k == 0:\n",
        "                    #get highest nonzero for pile behind card\n",
        "                    highest_nonzeros_piles_behind = np.where(np.count_nonzero(self.pile_behind_cards, axis=0)==0, -1, (self.pile_behind_cards.shape[0]-1) - np.argmin(self.pile_behind_cards[::-1,:]==0, axis=0))[pile_i]\n",
        "                    if highest_nonzeros_piles_behind > -1:\n",
        "                        self.pile_cards[0, pile_i] = self.pile_behind_cards[highest_nonzeros_piles_behind,pile_i]\n",
        "                        self.pile_behind_cards[highest_nonzeros_piles_behind, pile_i] = 0\n",
        "                        self.pile_behind_cards_known[highest_nonzeros_piles_behind, pile_i] = 0\n",
        "                        game_reward += game_score_pile_card_reveal\n",
        "                        reward += agent_reward_pile_card_reveal\n",
        "\n",
        "        self.game_episode_rewards += game_reward\n",
        "        self.agent_episode_rewards += reward\n",
        "        #try if agent rewards get too low, just terminate\n",
        "        self.episode_steps += 1\n",
        "        if self.episode_steps > 350:\n",
        "            terminated = True\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        # return self._get_obs(), reward, terminated, False, {\"action_mask\": self.action_mask(self.deck_cards, self.suit_cards, self.pile_cards)}\n",
        "        return self._get_obs(), reward, terminated, {\"action_mask\": self.action_mask(self.deck_cards, self.suit_cards, self.pile_cards)}\n",
        "\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return {\n",
        "            \"deck_position\": self.deck_position,\n",
        "            \"decks_actual\": self.deck_cards,\n",
        "            \"decks\": self.deck_cards_known,\n",
        "            \"suits\": self.suit_cards,\n",
        "            \"piles\": self.pile_cards,\n",
        "            \"piles_behind\": self.pile_behind_cards_known,\n",
        "            \"piles_behind_actual\": self.pile_behind_cards\n",
        "        }\n",
        "        #return (sum_hand(self.player), self.dealer[0], usable_ace(self.player))\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None,\n",
        "    ):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        shuffled_deck = get_shuffled_deck(self.np_random)\n",
        "\n",
        "        self.game_episode_rewards = 0\n",
        "        self.agent_episode_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "\n",
        "        self.deck_cards = shuffled_deck[:24].reshape((8,3))\n",
        "        self.deck_cards_known = np.zeros((8,3), dtype=np.int8)\n",
        "        self.deck_cards_known.fill(53)\n",
        "        self.deck_cards_known[0,:] = self.deck_cards[0,:]\n",
        "        #testing\n",
        "        # self.deck_cards[0,2] = 9\n",
        "        # self.deck_cards_known[0,2] = 9\n",
        "        # self.deck_cards[0,1] = 15\n",
        "        # self.deck_cards_known[0,1] = 15\n",
        "        # self.deck_cards[0,0] = 28\n",
        "        # self.deck_cards_known[0,0] = 28\n",
        "\n",
        "        self.deck_position = 0\n",
        "\n",
        "        self.suit_cards = np.zeros(4, dtype=np.int8)\n",
        "        #testing\n",
        "        # self.suit_cards[0] = 3\n",
        "        # self.suit_cards[1] = 3\n",
        "        # self.suit_cards[2] = 3\n",
        "        # self.suit_cards[3] = 3\n",
        "\n",
        "        #NOTE: SHOULD try to MAKE THIS MORE EFFICIENT\n",
        "        self.pile_cards = np.zeros((13,7), dtype=np.int8)\n",
        "        self.pile_behind_cards_known = np.zeros((6,7), dtype=np.int8)\n",
        "        self.pile_behind_cards = np.zeros((6,7), dtype=np.int8)\n",
        "        card_index = 24\n",
        "        for i in range(7):\n",
        "            bottom_pile_card = get_suit_and_num(shuffled_deck[card_index])\n",
        "            self.pile_cards[0,i] = shuffled_deck[card_index]\n",
        "            card_index += 1\n",
        "\n",
        "            for j in range(i):\n",
        "                self.pile_behind_cards_known[j,i] = 53\n",
        "                self.pile_behind_cards[j,i] = shuffled_deck[card_index]\n",
        "                card_index += 1\n",
        "\n",
        "        #testing\n",
        "        # self.pile_cards[0,0] = 0\n",
        "        # self.pile_cards[1,0] = 0\n",
        "        # self.pile_cards[2,0] = 0\n",
        "        # self.pile_cards[3,0] = 0\n",
        "        # self.pile_cards[4,0] = 0\n",
        "        # self.pile_cards[5,0] = 0\n",
        "        # self.pile_cards[6,0] = 0\n",
        "        # self.pile_cards[7,0] = 0\n",
        "        # self.pile_cards[8,0] = 0\n",
        "        # self.pile_cards[9,0] = 0\n",
        "        # self.pile_cards[10,0] = 0\n",
        "        # self.pile_cards[11,0] = 0\n",
        "        # self.pile_cards[12,0] = 0\n",
        "\n",
        "        # self.pile_cards[0,1] = 33\n",
        "        # self.pile_cards[1,1] = 8\n",
        "        # self.pile_cards[2,1] = 35\n",
        "        # self.pile_cards[3,1] = 10\n",
        "        # self.pile_cards[4,1] = 0\n",
        "        # self.pile_cards[5,1] = 0\n",
        "        # self.pile_cards[6,1] = 0\n",
        "        # self.pile_cards[7,1] = 0\n",
        "        # self.pile_cards[8,1] = 0\n",
        "        # self.pile_cards[9,1] = 0\n",
        "        # self.pile_cards[10,1] = 0\n",
        "        # self.pile_cards[11,1] = 0\n",
        "        # self.pile_cards[12,1] = 0\n",
        "\n",
        "        # self.pile_behind_cards[0,1] = 0\n",
        "        # self.pile_behind_cards_known[0,1] = 0\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return self._get_obs(), {\"action_mask\": self.action_mask(self.deck_cards, self.suit_cards, self.pile_cards)}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode is None:\n",
        "            gym.logger.warn(\n",
        "                \"You are calling render method without specifying any render mode. \"\n",
        "                \"You can specify the render_mode at initialization, \"\n",
        "                f'e.g. gym(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
        "            )\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            import pygame\n",
        "        except ImportError:\n",
        "            raise DependencyNotInstalled(\n",
        "                \"pygame is not installed, run `pip install gym[toy_text]`\"\n",
        "            )\n",
        "\n",
        "        #player_sum, dealer_card_value, usable_ace = self._get_obs()\n",
        "        screen_width, screen_height = 1200, 640\n",
        "        card_img_height = screen_height // 4\n",
        "        card_img_width = int(card_img_height * 142 / 197)\n",
        "        spacing = screen_height // 20\n",
        "\n",
        "        bg_color = (7, 99, 36)\n",
        "        white = (255, 255, 255)\n",
        "\n",
        "        if not hasattr(self, \"screen\"):\n",
        "            pygame.init()\n",
        "            if self.render_mode == \"human\":\n",
        "                pygame.display.init()\n",
        "                self.screen = pygame.display.set_mode((screen_width, screen_height))\n",
        "            else:\n",
        "                pygame.font.init()\n",
        "                self.screen = pygame.Surface((screen_width, screen_height))\n",
        "\n",
        "        if not hasattr(self, \"clock\"):\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        self.screen.fill(bg_color)\n",
        "\n",
        "        def get_image(path):\n",
        "            cwd = os.path.dirname(__file__)\n",
        "            image = pygame.image.load(os.path.join(cwd, path))\n",
        "            return image\n",
        "\n",
        "        def get_font(path, size):\n",
        "            cwd = os.path.dirname(__file__)\n",
        "            font = pygame.font.Font(os.path.join(cwd, path), size)\n",
        "            return font\n",
        "\n",
        "        small_font = get_font(\n",
        "            os.path.join(\"font\", \"Minecraft.ttf\"), screen_height // 15\n",
        "        )\n",
        "\n",
        "        score_text = small_font.render(\n",
        "            \"Score: \" + str(self.game_episode_rewards), True, white\n",
        "        )\n",
        "        score_text_rect = self.screen.blit(score_text, (spacing, 0))\n",
        "\n",
        "        reward_text = small_font.render(\n",
        "            \"Reward: \" + str(self.agent_episode_rewards), True, white\n",
        "        )\n",
        "        reward_text_rect = self.screen.blit(reward_text, (screen_width // 2, 0))\n",
        "\n",
        "        def scale_card_img(card_img):\n",
        "            return pygame.transform.scale(card_img, (card_img_width, card_img_height))\n",
        "\n",
        "\n",
        "        #UPSIDE DOWN DECK CARD\n",
        "        hidden_card_img = scale_card_img(get_image(os.path.join(\"img\", \"Card.png\")))\n",
        "        self.screen.blit(\n",
        "            hidden_card_img,\n",
        "            (\n",
        "                0,\n",
        "                score_text_rect.bottom + spacing,\n",
        "            ),\n",
        "        )\n",
        "        #DECK CARDS\n",
        "        if self.deck_cards[0,0] > 0:\n",
        "            deck_card_one_suit_and_num = get_suit_and_num(self.deck_cards[0,0])\n",
        "            deck_card_one_suit_char = get_suit_char_from_val(deck_card_one_suit_and_num[0])\n",
        "            deck_card_one_num_char = get_card_char(deck_card_one_suit_and_num[1])\n",
        "            deck_card_one_img = scale_card_img(\n",
        "                get_image(\n",
        "                    os.path.join(\n",
        "                        \"img\",\n",
        "                        f\"{deck_card_one_suit_char}{deck_card_one_num_char}.png\",\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            deck_card_one_rect = self.screen.blit(\n",
        "                deck_card_one_img,\n",
        "                (\n",
        "                    card_img_width + spacing,\n",
        "                    score_text_rect.bottom + spacing,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        if self.deck_cards[0,1] > 0:\n",
        "            deck_card_two_suit_and_num = get_suit_and_num(self.deck_cards[0,1])\n",
        "            deck_card_two_suit_char = get_suit_char_from_val(deck_card_two_suit_and_num[0])\n",
        "            deck_card_two_num_char = get_card_char(deck_card_two_suit_and_num[1])\n",
        "            deck_card_two_img = scale_card_img(\n",
        "                get_image(\n",
        "                    os.path.join(\n",
        "                        \"img\",\n",
        "                        f\"{deck_card_two_suit_char}{deck_card_two_num_char}.png\",\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            deck_card_two_rect = self.screen.blit(\n",
        "                deck_card_two_img,\n",
        "                (\n",
        "                    1.5*card_img_width + spacing,\n",
        "                    score_text_rect.bottom + spacing,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        if self.deck_cards[0,2] > 0:\n",
        "            deck_card_three_suit_and_num = get_suit_and_num(self.deck_cards[0,2])\n",
        "            deck_card_three_suit_char = get_suit_char_from_val(deck_card_three_suit_and_num[0])\n",
        "            deck_card_three_num_char = get_card_char(deck_card_three_suit_and_num[1])\n",
        "            deck_card_three_img = scale_card_img(\n",
        "                get_image(\n",
        "                    os.path.join(\n",
        "                        \"img\",\n",
        "                        f\"{deck_card_three_suit_char}{deck_card_three_num_char}.png\",\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "            deck_card_three_rect = self.screen.blit(\n",
        "                deck_card_three_img,\n",
        "                (\n",
        "                    2*card_img_width + spacing,\n",
        "                    score_text_rect.bottom + spacing,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "\n",
        "        #SUIT CARDS\n",
        "        if self.suit_cards[3] > 0:\n",
        "            suit_card_four_suit_char = get_suit_char_from_val(3)\n",
        "            suit_card_four_num_char = get_card_char(self.suit_cards[3])\n",
        "            suit_card_four_img = scale_card_img(get_image(os.path.join(\"img\", \"Card.png\")))\n",
        "            if suit_card_four_suit_char != \"Card\":\n",
        "                suit_card_four_img = scale_card_img(\n",
        "                    get_image(\n",
        "                        os.path.join(\n",
        "                            \"img\",\n",
        "                            f\"{suit_card_four_suit_char}{suit_card_four_num_char}.png\",\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "            self.screen.blit(\n",
        "                suit_card_four_img,\n",
        "                (\n",
        "                    screen_width - card_img_width,\n",
        "                    score_text_rect.bottom + spacing,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        if self.suit_cards[2] > 0:\n",
        "            suit_card_three_suit_char = get_suit_char_from_val(2)\n",
        "            suit_card_three_num_char = get_card_char(self.suit_cards[2])\n",
        "            suit_card_three_img = scale_card_img(get_image(os.path.join(\"img\", \"Card.png\")))\n",
        "            if suit_card_three_suit_char != \"Card\":\n",
        "                suit_card_three_img = scale_card_img(\n",
        "                    get_image(\n",
        "                        os.path.join(\n",
        "                            \"img\",\n",
        "                            f\"{suit_card_three_suit_char}{suit_card_three_num_char}.png\",\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "            self.screen.blit(\n",
        "                suit_card_three_img,\n",
        "                (\n",
        "                    screen_width - 2*card_img_width,\n",
        "                    score_text_rect.bottom + spacing,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        if self.suit_cards[1] > 0:\n",
        "            suit_card_two_suit_char = get_suit_char_from_val(1)\n",
        "            suit_card_two_num_char = get_card_char(self.suit_cards[1])\n",
        "            suit_card_two_img = scale_card_img(get_image(os.path.join(\"img\", \"Card.png\")))\n",
        "            if suit_card_two_suit_char != \"Card\":\n",
        "                suit_card_two_img = scale_card_img(\n",
        "                    get_image(\n",
        "                        os.path.join(\n",
        "                            \"img\",\n",
        "                            f\"{suit_card_two_suit_char}{suit_card_two_num_char}.png\",\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "            self.screen.blit(\n",
        "                suit_card_two_img,\n",
        "                (\n",
        "                    screen_width - 3*card_img_width,\n",
        "                    score_text_rect.bottom + spacing,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        if self.suit_cards[0] > 0:\n",
        "            suit_card_one_suit_char = get_suit_char_from_val(0)\n",
        "            suit_card_one_num_char = get_card_char(self.suit_cards[0])\n",
        "            suit_card_one_img = scale_card_img(get_image(os.path.join(\"img\", \"Card.png\")))\n",
        "            if suit_card_one_suit_char != \"Card\":\n",
        "                suit_card_one_img = scale_card_img(\n",
        "                    get_image(\n",
        "                        os.path.join(\n",
        "                            \"img\",\n",
        "                            f\"{suit_card_one_suit_char}{suit_card_one_num_char}.png\",\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "            self.screen.blit(\n",
        "                suit_card_one_img,\n",
        "                (\n",
        "                    screen_width - 4*card_img_width,\n",
        "                    score_text_rect.bottom + spacing,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "\n",
        "        #PILE CARDS\n",
        "\n",
        "        for i in range(int(self.pile_cards.shape[1])):\n",
        "            spacing_counter = 0\n",
        "            #place behind cards\n",
        "            for j in range(6):\n",
        "                if self.pile_behind_cards[j,i] != 0:\n",
        "                    pile_card_img = scale_card_img(get_image(os.path.join(\"img\", \"Card.png\")))\n",
        "                    self.screen.blit(\n",
        "                        pile_card_img,\n",
        "                        (\n",
        "                            i * (card_img_width + 50),\n",
        "                            (screen_height // 2.75) + (spacing // 2) + spacing_counter,\n",
        "                        ),\n",
        "                    )\n",
        "                    spacing_counter += 10\n",
        "\n",
        "            for j in range(12,-1,-1):\n",
        "                if self.pile_cards[j,i] > 0:\n",
        "                    pile_card_suit_and_num = get_suit_and_num(self.pile_cards[j,i])\n",
        "                    pile_card_suit_char = get_suit_char_from_val(pile_card_suit_and_num[0])\n",
        "                    pile_card_num_char = get_card_char(pile_card_suit_and_num[1])\n",
        "                    #pile_card_img = scale_card_img(get_image(os.path.join(\"img\", \"Card.png\")))\n",
        "                    #if pile_card_suit_char != \"Card\":\n",
        "                    pile_card_img = scale_card_img(\n",
        "                        get_image(\n",
        "                            os.path.join(\n",
        "                                \"img\",\n",
        "                                f\"{pile_card_suit_char}{pile_card_num_char}.png\",\n",
        "                            )\n",
        "                        )\n",
        "                    )\n",
        "                    self.screen.blit(\n",
        "                        pile_card_img,\n",
        "                        (\n",
        "                            i * (card_img_width + 50),\n",
        "                            (screen_height // 2.75) + (spacing // 2) + spacing_counter,\n",
        "                        ),\n",
        "                    )\n",
        "                    spacing_counter += 30\n",
        "\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        else:\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "\n",
        "    def close(self):\n",
        "        if hasattr(self, \"screen\"):\n",
        "            import pygame\n",
        "\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()\n"
      ],
      "metadata": {
        "id": "7c6fhxuZEDCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN class"
      ],
      "metadata": {
        "id": "5B8zTDmLEB-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_channels = 21"
      ],
      "metadata": {
        "id": "gAJ6_hGcvPe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return torch.nn.Conv2d(\n",
        "        in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "    )"
      ],
      "metadata": {
        "id": "uSvhzxAijIVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(torch.nn.Module):\n",
        "    def __init__(self, num_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv3x3(num_channels, num_channels, stride)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(num_channels)\n",
        "        self.conv2 = conv3x3(num_channels, num_channels)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += x\n",
        "        out = torch.nn.functional.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "XZp0ETjAjEwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_actions):\n",
        "        super().__init__()\n",
        "        # self.conv1 = nn.Conv2d(num_channels, num_channels, 1)\n",
        "        # self.pool = nn.MaxPool2d(2, 2)\n",
        "        # self.conv2 = nn.Conv2d(num_channels, num_channels, 1)\n",
        "        # self.fc1 = nn.Linear(num_channels * 5 * 1, 120)\n",
        "        # self.fc2 = nn.Linear(120, 84)\n",
        "        # self.fc3 = nn.Linear(84, n_actions)\n",
        "        self.conv1 = nn.Conv2d(num_channels, num_channels // 2, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(num_channels // 2)\n",
        "        # self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(num_channels // 2, num_channels // 4, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(num_channels // 4)\n",
        "        self.fc1 = nn.Linear((num_channels // 4) * 23 * 7, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        self.fc3 = nn.Linear(512, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(F.relu(self.conv1(x)))\n",
        "        x = self.bn2(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "oKfkSYqNWjlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Memory class\n",
        "credit Pytorch official docs"
      ],
      "metadata": {
        "id": "KtsAz8rfW1cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "_u2yFfkOELYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matplotlib Setup"
      ],
      "metadata": {
        "id": "7gYE1bRmXYRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()"
      ],
      "metadata": {
        "id": "sUnf3sIPETEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters, initialization variables"
      ],
      "metadata": {
        "id": "s1aq_GiWXbB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
        "# GAMMA is the discount factor as mentioned in the previous section\n",
        "# EPS_START is the starting value of epsilon\n",
        "# EPS_END is the final value of epsilon\n",
        "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
        "# TAU is the update rate of the target network\n",
        "# LR is the learning rate of the AdamW optimizer\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.1\n",
        "# EPS_DECAY = 50000 #0.2 around 220\n",
        "EPS_DECAY = 10000 #0.1 around 140, #0.05 around 140\n",
        "TAU = 0.001\n",
        "LR = 1e-7\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "EkVm_uU_EUhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate Solitaire Gym environment"
      ],
      "metadata": {
        "id": "ZicgJwt0Xqzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = SolitaireWorldEnv()\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "# Get the number of state observations\n",
        "if gym.__version__[:4] == '0.26':\n",
        "    state, _ = env.reset()\n",
        "elif gym.__version__[:4] == '0.25':\n",
        "    state, _ = env.reset()"
      ],
      "metadata": {
        "id": "gxAvhfNmEWL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define functions to get suit and card number, vectorized"
      ],
      "metadata": {
        "id": "n4j3-3e8Xyik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_suit_val(card_param):\n",
        "    if card_param == 0:\n",
        "        return 0\n",
        "    elif card_param == 53:\n",
        "        return -1\n",
        "    \n",
        "    if card_param / 13 <= 1:\n",
        "        return 1\n",
        "    elif card_param / 26 <= 1:\n",
        "        return 2\n",
        "    elif card_param / 39 <= 1:\n",
        "        return 3\n",
        "    else:\n",
        "        return 4"
      ],
      "metadata": {
        "id": "dci4JvTAE9Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_card_val(card_p):\n",
        "    if card_p == 0:\n",
        "        return 0\n",
        "    elif card_p == 53:\n",
        "        return -1\n",
        "\n",
        "    if card_p % 13 == 0:\n",
        "        return 13\n",
        "    else:\n",
        "        return card_p % 13"
      ],
      "metadata": {
        "id": "Rgz2mchoE8tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suit_val_vec = np.vectorize(get_suit_val)\n",
        "card_val_vec = np.vectorize(get_card_val)"
      ],
      "metadata": {
        "id": "kmewBuzhE7Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define observation from Solitaire gym env, and properties"
      ],
      "metadata": {
        "id": "2_AW-M42X56U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_obs(obs):\n",
        "#     deck_num = obs[\"deck_position\"]\n",
        "#     suits = obs['suits']\n",
        "#     #first 2 rows, all columns of deck\n",
        "#     decks = obs['decks_actual'].flatten()\n",
        "#     decks_suits = suit_val_vec(decks)\n",
        "#     decks_card_vals = card_val_vec(decks)\n",
        "#     piles = obs['piles'].flatten()\n",
        "#     piles_suits = suit_val_vec(piles)\n",
        "#     piles_card_vals = card_val_vec(piles)\n",
        "#     piles_behind = (obs['piles_behind'] != 0).sum(0)\n",
        "#     return np.concatenate((np.array([deck_num]),suits,decks_suits,decks_card_vals,piles_suits,piles_card_vals,piles_behind))"
      ],
      "metadata": {
        "id": "YYUqfDQuFGaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# state = get_obs(state)"
      ],
      "metadata": {
        "id": "cOXZ-uqbFje2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_observations = len(state)"
      ],
      "metadata": {
        "id": "zpLX413IEpen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_observations"
      ],
      "metadata": {
        "id": "NPfTtkonokF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_three_d_obs(obs):\n",
        "    deck_num = obs[\"deck_position\"]\n",
        "\n",
        "    decks_me = obs['decks'].flatten()\n",
        "    top_deck_rows = decks_me[3:].reshape(3,7)\n",
        "\n",
        "    suits_me = obs['suits'].flatten()\n",
        "    #retrieve original card numbers from suits\n",
        "    #since they are encoded only 0-13, with 0\n",
        "    #being empty and 1-13 being card num, but\n",
        "    #diamond four is 17, but encoded as 4 in the\n",
        "    #index[1] aka index 2\n",
        "    if suits_me[1] > 0:\n",
        "        suits_me[1] = 13 + suits_me[1]\n",
        "    if suits_me[2] > 0:\n",
        "        suits_me[2] = (13*2) + suits_me[2]\n",
        "    if suits_me[3] > 0:\n",
        "        suits_me[3] = (13*3) + suits_me[3]\n",
        "    middle_deck_suit_row = np.concatenate((decks_me[:3],suits_me), axis=None).reshape(1,7)\n",
        "\n",
        "    piles_me = obs['piles']\n",
        "    piles_behind_me = obs['piles_behind']\n",
        "\n",
        "    full_array = np.vstack((top_deck_rows,middle_deck_suit_row,piles_me,piles_behind_me))\n",
        "\n",
        "    aces_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    twos_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    threes_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    fours_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    fives_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    sixes_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    sevens_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    eights_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    nines_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    tens_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    jacks_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    queens_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    kings_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    hearts_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    diamonds_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    spades_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    clubs_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    empty_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    unknown_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "    deck_num_channel = np.zeros_like(full_array, dtype=np.int8)\n",
        "\n",
        "\n",
        "\n",
        "    aces_channel[(full_array == 1) | (full_array == 14) | (full_array == 27) | (full_array == 40)] = 1\n",
        "    twos_channel[(full_array == 2) | (full_array == 15) | (full_array == 28) | (full_array == 41)] = 1\n",
        "    threes_channel[(full_array == 3) | (full_array == 16) | (full_array == 29) | (full_array == 42)] = 1\n",
        "    fours_channel[(full_array == 4) | (full_array == 17) | (full_array == 30) | (full_array == 43)] = 1\n",
        "    fives_channel[(full_array == 5) | (full_array == 18) | (full_array == 31) | (full_array == 44)] = 1\n",
        "    sixes_channel[(full_array == 6) | (full_array == 19) | (full_array == 32) | (full_array == 45)] = 1\n",
        "    sevens_channel[(full_array == 7) | (full_array == 20) | (full_array == 33) | (full_array == 46)] = 1\n",
        "    eights_channel[(full_array == 8) | (full_array == 21) | (full_array == 34) | (full_array == 47)] = 1\n",
        "    nines_channel[(full_array == 9) | (full_array == 22) | (full_array == 35) | (full_array == 48)] = 1\n",
        "    tens_channel[(full_array == 10) | (full_array == 23) | (full_array == 36) | (full_array == 49)] = 1\n",
        "    jacks_channel[(full_array == 11) | (full_array == 24) | (full_array == 37) | (full_array == 50)] = 1\n",
        "    queens_channel[(full_array == 12) | (full_array == 25) | (full_array == 38) | (full_array == 51)] = 1\n",
        "    kings_channel[(full_array == 13) | (full_array == 26) | (full_array == 39) | (full_array == 52)] = 1\n",
        "\n",
        "    hearts_channel[(full_array >= 1) & (full_array <= 13)] = 1\n",
        "    diamonds_channel[(full_array >= 14) & (full_array <= 26)] = 1\n",
        "    spades_channel[(full_array >= 27) & (full_array <= 39)] = 1\n",
        "    clubs_channel[(full_array >= 40) & (full_array <= 52)] = 1\n",
        "\n",
        "    empty_channel[full_array == 0] = 1\n",
        "    unknown_channel[full_array == 53] = 1\n",
        "\n",
        "    if deck_num < 7:\n",
        "        deck_num_channel[0, deck_num] = 1\n",
        "    else:\n",
        "        deck_num_channel[22, 6] = 1\n",
        "\n",
        "\n",
        "    ones_cnn_channel = np.ones_like(full_array, dtype=np.int8)\n",
        "\n",
        "\n",
        "    obs_me = np.array([aces_channel,twos_channel,threes_channel,\n",
        "            fours_channel,fives_channel,sixes_channel,\n",
        "            sevens_channel,eights_channel,nines_channel,\n",
        "            tens_channel,jacks_channel,queens_channel,\n",
        "            kings_channel,hearts_channel,diamonds_channel,\n",
        "            spades_channel,clubs_channel,empty_channel,\n",
        "            unknown_channel,deck_num_channel,ones_cnn_channel])\n",
        "    return obs_me"
      ],
      "metadata": {
        "id": "4-KFUY4Cqo76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_test = get_three_d_obs(state)"
      ],
      "metadata": {
        "id": "SOGQhXoorfe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdCPnrLvrkrY",
        "outputId": "2adb5f85-f879-42c8-b5d6-52ba4e40c998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21, 23, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 302
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instaniate Policy and Target DQN networks, and replay memory buffer"
      ],
      "metadata": {
        "id": "cPK-qUUkYGpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "I6jK5B99oiuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# policy_net = DQN(n_observations, n_actions).to(device)\n",
        "policy_net = DQN(n_actions).to(device)\n",
        "#my, set weights to 0 because could be messing with fact legal actions are sparse?\n",
        "# policy_net.apply(init_weights)\n",
        "# target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "steps_done = 0"
      ],
      "metadata": {
        "id": "B1Mpx9qMEY-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define select_action function, for epsilon-greedy action selection"
      ],
      "metadata": {
        "id": "THJHITdQYn9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ],
      "metadata": {
        "id": "gcxXpuUACUSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.dirichlet([10] * 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e0xFpc8NjxP",
        "outputId": "3592a686-1879-45ac-f9ca-62f56d6d6ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.48668237, 0.51331763])"
            ]
          },
          "metadata": {},
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state, legal_actions):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    # print(eps_threshold)\n",
        "    # if eps_threshold > 0.795 and eps_threshold < 0.805:\n",
        "    #     print(eps_threshold)\n",
        "    # if eps_threshold > 0.695 and eps_threshold < 0.705:\n",
        "    #     print(eps_threshold)\n",
        "    # if eps_threshold > 0.595 and eps_threshold < 0.605:\n",
        "    #     print(eps_threshold)\n",
        "    # if eps_threshold > 0.495 and eps_threshold < 0.505:\n",
        "    #     print(eps_threshold)\n",
        "    # if eps_threshold > 0.395 and eps_threshold < 0.405:\n",
        "    #     print(eps_threshold)\n",
        "    # if eps_threshold > 0.295 and eps_threshold < 0.305:\n",
        "    #     print(eps_threshold)\n",
        "    # if eps_threshold > 0.195 and eps_threshold < 0.205:\n",
        "    #     print(eps_threshold)\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            q_legal_max = None\n",
        "            if eps_threshold < EPS_END + 0.001:\n",
        "                # print(\"HIT!\")\n",
        "                q_vals_all_actions = policy_net(state)[0]\n",
        "                q_vals_legal_actions = q_vals_all_actions[legal_actions]\n",
        "                # print(q_vals_all_actions)\n",
        "                # print(q_vals_all_actions[44])\n",
        "                # # print(\"q_vals_legal_actions: \", q_vals_legal_actions)\n",
        "                noise = np.random.dirichlet([10] * q_vals_legal_actions.size()[0])\n",
        "                # # print(\"noise\", noise)\n",
        "                noise = noise * 3\n",
        "                # # print(\"noise after\", noise)\n",
        "                q_vals_legal_with_noise = q_vals_legal_actions + torch.tensor(noise).to(\"cuda:0\")\n",
        "                # # print(\"q_vals_legal_with_noise\", q_vals_legal_with_noise)\n",
        "                q_with_noise_max = q_vals_legal_with_noise.max(0)\n",
        "                # # print(\"q with noise max\", q_with_noise_max)\n",
        "                # q_with_noise_max_value = q_with_noise_max[0].cpu().numpy()\n",
        "                # # print(\"q with noise max value\", q_with_noise_max_value)\n",
        "                q_with_noise_max_index = q_with_noise_max[1].cpu().numpy()\n",
        "                # # print(\"q with noise max index\", q_with_noise_max_index)\n",
        "                q_legal_max = float(q_vals_legal_actions[q_with_noise_max_index])\n",
        "                # # print(\"q_legal_max\", q_legal_max)\n",
        "                # q_vals_all_actions = policy_net(state)[0]\n",
        "                # q_vals_legal_actions = q_vals_all_actions[legal_actions]\n",
        "                # q_legal_max = float(q_vals_legal_actions.max(0)[0].cpu().numpy())\n",
        "            else:\n",
        "                # t.max(1) will return largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "                #my:\n",
        "                #get all q values\n",
        "                q_vals_all_actions = policy_net(state)[0]\n",
        "                # to check they are all initially low\n",
        "                # print(q_vals_all_actions)\n",
        "                #subset tensor of just legal actions\n",
        "                q_vals_legal_actions = q_vals_all_actions[legal_actions]\n",
        "                #get max (value, to check for index in orig later)\n",
        "                #of legal actions tensor\n",
        "                q_legal_max = float(q_vals_legal_actions.max(0)[0].cpu().numpy())\n",
        "            # print(\"q_legal_max going in\", q_legal_max)\n",
        "            max_val_tensor = (q_vals_all_actions == q_legal_max).nonzero(as_tuple=True)[0]\n",
        "            if max_val_tensor.size()[0] > 1:\n",
        "                for i in range(max_val_tensor.size()[0]):\n",
        "                    #check if index 0 matches any in q_vals_legal_actions\n",
        "                    if int(max_val_tensor[i].cpu().numpy()) in legal_actions:\n",
        "                        return torch.tensor([[int(max_val_tensor[i].cpu().numpy())]], device=device, dtype=torch.long)\n",
        "            return max_val_tensor.view(1,1)\n",
        "    else:\n",
        "        return torch.tensor([[np.random.choice(legal_actions,1)[0]]], device=device, dtype=torch.long)"
      ],
      "metadata": {
        "id": "FWJKhbYiEanI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation, info = env.reset()\n",
        "# observation = get_obs(observation)\n",
        "observation = get_three_d_obs(observation)\n",
        "inf_legal_actions = info['action_mask'].nonzero()[0]"
      ],
      "metadata": {
        "id": "xEJ4P02SIK-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inf_legal_actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39f4gU-QKEAf",
        "outputId": "aba95a3f-8518-4265-dca8-94a39f54222a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0 260 380]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# observation = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "observation = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)"
      ],
      "metadata": {
        "id": "Jeh_dyFwIMx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observation.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1i8YzzEwS1M",
        "outputId": "251bca08-62af-4f9b-873f-b127fac3ca3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 21, 23, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hmm = select_action(observation, inf_legal_actions)"
      ],
      "metadata": {
        "id": "H12UdvuaIUpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IfLtQPhkQBUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define plot function, initialize episode duration/score history array"
      ],
      "metadata": {
        "id": "Z-sPtNjNZJRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episode_durations = []"
      ],
      "metadata": {
        "id": "B1ch0kdOEb-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ],
      "metadata": {
        "id": "PLoI-W9gEdZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define optimize_model function for performing a gradient update step"
      ],
      "metadata": {
        "id": "OuhmcTn5ZTVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define input tuple for a batch of transitions from the replay memory buffer\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'info_action_mask'))"
      ],
      "metadata": {
        "id": "RtJQnhhLZgNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#performs gradient update step\n",
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    \n",
        "    # print(\"batch next state: \", batch.next_state[0].size())\n",
        "    # print(\"len next state: \", len(batch.next_state))\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    \n",
        "    # print(\"non final next states size: \", non_final_next_states.size())\n",
        "    mask_batch = torch.cat([s for i, s in enumerate(batch.info_action_mask) if batch.next_state[i] is not None])\n",
        "    # print(\"mask batch: \", mask_batch)\n",
        "    # print('len mask', len(batch.info_action_mask))\n",
        "    # print('size mask batch: ', mask_batch.size())\n",
        "    # mask_batch = torch.cat([s for s in batch.info_action_mask\n",
        "    #                                             if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    \n",
        "    # mask_batch = torch.cat(batch.info_action_mask)\n",
        "    # print(mask_batch)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        #get all the q values from the target net for nonfinal states\n",
        "        target_all_nonfinal_qs = target_net(non_final_next_states)\n",
        "        #multiply it by the boolean legal action mask for this next state\n",
        "        # my_test = target_net(non_final_next_states)\n",
        "        # print(\"len my test: \", len(my_test))\n",
        "        # print(\"target pred qs: \", my_test)\n",
        "        # print(\"after max(1): \", my_test.max(1))\n",
        "        #note, the size is still the same, it's just illegal ones now\n",
        "        #are set to zero, via the multiplication over the same shaped\n",
        "        #boolean mask, mask_batch\n",
        "        target_legal_nonfinal_qs = torch.mul(target_all_nonfinal_qs, mask_batch)\n",
        "        # my_test_after_mul = torch.mul(my_test, mask_batch)\n",
        "        # print(\"multiply: \", my_test_after_mul)\n",
        "        # my_test_after_mul[my_test_after_mul == 0] = float('-inf')\n",
        "        #set 0s to -inf, because zero could be the highest and it could\n",
        "        #be an illegal action and we dont want that. Sometimes an action\n",
        "        #will have only negative reward, and if we leave them as zeros\n",
        "        #that will become max\n",
        "        target_legal_nonfinal_qs[target_legal_nonfinal_qs == 0] = float('-inf')\n",
        "        # print(\"no way: \", my_test_after_mul)\n",
        "        # my_test_after_max = my_test_after_mul.max(1)[0]\n",
        "        # print(my_test_after_max)\n",
        "        # print(\"mytest[mask_batch]: \", my_test[mask_batch])\n",
        "        next_state_values[non_final_mask] = target_legal_nonfinal_qs.max(1)[0]\n",
        "        # print(next_state_values)\n",
        "        # next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "    \n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 0.1)\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "84t-ExDjEe5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute DQN algorithm, continuously plotting rewards"
      ],
      "metadata": {
        "id": "wq1tn2_maGDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    num_episodes = 500\n",
        "else:\n",
        "    num_episodes = 100\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    #my\n",
        "    episode_reward = 0\n",
        "    # Initialize the environment and get it's state\n",
        "    if gym.__version__[:4] == '0.26':\n",
        "        state, info = env.reset()\n",
        "    elif gym.__version__[:4] == '0.25':\n",
        "        state, info = env.reset()\n",
        "    # state = get_obs(state)\n",
        "    state = get_three_d_obs(state)\n",
        "    info_legal_actions = info['action_mask'].nonzero()[0]\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    for t in count():\n",
        "        action = select_action(state, info_legal_actions)\n",
        "        observation, reward, terminated, info = env.step(action.item())\n",
        "        #my\n",
        "        # observation = get_obs(observation)\n",
        "        observation = get_three_d_obs(observation)\n",
        "        info_legal_actions = info['action_mask'].nonzero()[0]\n",
        "        episode_reward += reward\n",
        "\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward, torch.tensor(info['action_mask'], device=device, dtype=torch.bool).view(1,-1))\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "        \n",
        "        if done:\n",
        "            episode_durations.append(episode_reward)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "print('Complete')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "NTVSFQ3WEgmV",
        "outputId": "1ff3bf51-fff9-4db1-c4dd-28f8ebf20c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-325-75c4b00e4de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the policy network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Soft update of the target network's weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-324-03141fad5ba0>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;31m# In-place gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(policy_net.state_dict(), \"mostrecentmeh.pt\")"
      ],
      "metadata": {
        "id": "jSX8ZnRQEiRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z2P30Hb4PdK0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}